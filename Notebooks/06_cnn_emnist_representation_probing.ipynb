{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51779fda",
   "metadata": {},
   "source": [
    "# Artificial Vision & Feature Separability — 06 · CNN on EMNIST + Representation Probing\n",
    "\n",
    "**Goal.** Train a small CNN on EMNIST (letters/digits), then **probe internal representations** with PCA and linear probes to study feature separability across layers.\n",
    "\n",
    "**Outputs.** Training curves, confusion matrix, example predictions, **PCA plots of layer activations**, and linear-probe accuracies (per layer).\n",
    "\n",
    "**Data.** `torchvision.datasets.EMNIST` (byclass split). Use SSL fix below if downloads fail locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d26f6419",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Reproducibility & Environment (with SSL fix) ---\n",
    "import os, random, numpy as np, certifi, torch\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "os.makedirs(\"results\", exist_ok=True)\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "\n",
    "# SSL fix so dataset downloads don't fail on some systems\n",
    "os.environ[\"SSL_CERT_FILE\"] = certifi.where()\n",
    "print(\"SSL_CERT_FILE set to:\", os.environ[\"SSL_CERT_FILE\"])\n",
    "print(\"Seed set to\", SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0feb5175",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Imports ---\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, log_loss, roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf8624f",
   "metadata": {},
   "source": [
    "## 1. Config\n",
    "Adjust batch size, epochs, and subset sizes for quick experimentation or full runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59304986",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cfg:\n",
    "    split = \"byclass\"    # EMNIST split: 'byclass' has 62 classes (10 digits + 52 letters)\n",
    "    batch_size = 128\n",
    "    epochs = 5           # increase (e.g., 10–20) for stronger accuracy\n",
    "    lr = 1e-3\n",
    "    train_subset = 20000   # set to None for full train\n",
    "    test_subset  = 5000    # set to None for full test\n",
    "    num_workers = 2\n",
    "\n",
    "cfg = Cfg()\n",
    "vars(cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f57213",
   "metadata": {},
   "source": [
    "## 2. Data — EMNIST (byclass)\n",
    "EMNIST images come rotated; we apply the recommended transform. We also optionally subset for speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a2461a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EMNIST images need to be transposed; torchvision provides 'transforms' guidance.\n",
    "# We'll convert to tensors and normalize to mean/std of EMNIST if desired; here we scale to [0,1].\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Lambda(lambda x: x.transpose(1,2).flip(2))  # rotate to upright\n",
    "])\n",
    "\n",
    "trainset = torchvision.datasets.EMNIST(root=\"./data\", split=cfg.split, train=True, download=True, transform=transform)\n",
    "testset  = torchvision.datasets.EMNIST(root=\"./data\", split=cfg.split, train=False, download=True, transform=transform)\n",
    "\n",
    "if cfg.train_subset is not None:\n",
    "    train_idx = list(range(min(cfg.train_subset, len(trainset))))\n",
    "    trainset = Subset(trainset, train_idx)\n",
    "\n",
    "if cfg.test_subset is not None:\n",
    "    test_idx = list(range(min(cfg.test_subset, len(testset))))\n",
    "    testset = Subset(testset, test_idx)\n",
    "\n",
    "train_loader = DataLoader(trainset, batch_size=cfg.batch_size, shuffle=True, num_workers=cfg.num_workers)\n",
    "test_loader  = DataLoader(testset,  batch_size=cfg.batch_size, shuffle=False, num_workers=cfg.num_workers)\n",
    "\n",
    "# Number of classes\n",
    "if isinstance(trainset, Subset):\n",
    "    n_classes = len(set([int(trainset.dataset.classes[i]) for i in range(len(trainset.dataset.classes))])) if hasattr(trainset.dataset, \"classes\") else 62\n",
    "else:\n",
    "    n_classes = len(trainset.classes) if hasattr(trainset, \"classes\") else 62\n",
    "print(\"Train batches:\", len(train_loader), \" Test batches:\", len(test_loader), \" Classes:\", n_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d62daf",
   "metadata": {},
   "source": [
    "## 3. Model — Small CNN with Named Layers\n",
    "We expose intermediate activations (`conv1`, `conv2`, `fc1`) for probing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95588e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SmallCNN(nn.Module):\n",
    "    def __init__(self, num_classes=62):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.pool  = nn.MaxPool2d(2,2)\n",
    "        self.drop  = nn.Dropout(0.25)\n",
    "        self.fc1   = nn.Linear(64*7*7, 128)\n",
    "        self.fc2   = nn.Linear(128, num_classes)\n",
    "    def forward(self, x, return_acts=False):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        a1 = x\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        a2 = x\n",
    "        x = self.pool(x)\n",
    "        x = self.drop(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        a3 = x\n",
    "        x = self.drop(x)\n",
    "        logits = self.fc2(x)\n",
    "        if return_acts:\n",
    "            return logits, {\"conv1\": a1, \"conv2\": a2, \"fc1\": a3}\n",
    "        return logits\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = SmallCNN(num_classes=n_classes).to(device)\n",
    "sum(p.numel() for p in model.parameters()), device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd3abc49",
   "metadata": {},
   "source": [
    "## 4. Train\n",
    "Standard cross-entropy training with Adam. We log **loss** and **accuracy** per epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db02618",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, loader, opt, loss_fn):\n",
    "    model.train()\n",
    "    total, correct, total_loss = 0, 0, 0.0\n",
    "    for imgs, labels in loader:\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "        opt.zero_grad()\n",
    "        logits = model(imgs)\n",
    "        loss = loss_fn(logits, labels)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        total += labels.size(0)\n",
    "        total_loss += float(loss.item()) * labels.size(0)\n",
    "        correct += (logits.argmax(1) == labels).sum().item()\n",
    "    return total_loss/total, correct/total\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader, loss_fn):\n",
    "    model.eval()\n",
    "    total, correct, total_loss = 0, 0, 0.0\n",
    "    for imgs, labels in loader:\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "        logits = model(imgs)\n",
    "        loss = loss_fn(logits, labels)\n",
    "        total += labels.size(0)\n",
    "        total_loss += float(loss.item()) * labels.size(0)\n",
    "        correct += (logits.argmax(1) == labels).sum().item()\n",
    "    return total_loss/total, correct/total\n",
    "\n",
    "import time\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "opt = torch.optim.Adam(model.parameters(), lr=cfg.lr)\n",
    "\n",
    "hist = {\"train_loss\": [], \"train_acc\": [], \"val_loss\": [], \"val_acc\": []}\n",
    "for ep in range(cfg.epochs):\n",
    "    t0 = time.time()\n",
    "    tl, ta = train_one_epoch(model, train_loader, opt, loss_fn)\n",
    "    vl, va = evaluate(model, test_loader, loss_fn)\n",
    "    hist[\"train_loss\"].append(tl); hist[\"train_acc\"].append(ta)\n",
    "    hist[\"val_loss\"].append(vl);   hist[\"val_acc\"].append(va)\n",
    "    print(f\"Epoch {ep+1:02d}/{cfg.epochs}  train_loss={tl:.4f}  train_acc={ta:.4f}  val_loss={vl:.4f}  val_acc={va:.4f}  ({time.time()-t0:.1f}s)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e26e6c",
   "metadata": {},
   "source": [
    "## 5. Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e38eb54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(hist[\"train_loss\"], label=\"train_loss\")\n",
    "plt.plot(hist[\"val_loss\"], label=\"val_loss\")\n",
    "plt.legend(); plt.title(\"Loss\"); plt.xlabel(\"epoch\"); plt.ylabel(\"loss\")\n",
    "plt.tight_layout(); plt.savefig(\"results/06_loss_curves.png\", dpi=150); plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(hist[\"train_acc\"], label=\"train_acc\")\n",
    "plt.plot(hist[\"val_acc\"], label=\"val_acc\")\n",
    "plt.legend(); plt.title(\"Accuracy\"); plt.xlabel(\"epoch\"); plt.ylabel(\"acc\")\n",
    "plt.tight_layout(); plt.savefig(\"results/06_acc_curves.png\", dpi=150); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e6a5112",
   "metadata": {},
   "source": [
    "## 6. Confusion Matrix & Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "335b18ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def collect_preds(model, loader):\n",
    "    model.eval()\n",
    "    all_logits, all_labels = [], []\n",
    "    for imgs, labels in loader:\n",
    "        imgs = imgs.to(device)\n",
    "        logits = model(imgs)\n",
    "        all_logits.append(logits.cpu())\n",
    "        all_labels.append(labels)\n",
    "    return torch.cat(all_logits,0).numpy(), torch.cat(all_labels,0).numpy()\n",
    "\n",
    "logits_test, y_test = collect_preds(model, test_loader)\n",
    "y_pred = logits_test.argmax(1)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print(f\"Test Accuracy: {acc:.4f}\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(cm, aspect=\"auto\")\n",
    "plt.title(\"EMNIST — Confusion Matrix (CNN)\")\n",
    "plt.xlabel(\"Pred\"); plt.ylabel(\"True\")\n",
    "plt.colorbar(); plt.tight_layout(); plt.savefig(\"results/06_confusion_cnn_emnist.png\", dpi=150); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab6b5bde",
   "metadata": {},
   "source": [
    "## 7. Representation Probing — PCA on Layer Activations\n",
    "We project **conv1**, **conv2**, and **fc1** activations to 2D and visualize separability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b5d0c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def collect_activations(model, loader, max_batches=50):\n",
    "    model.eval()\n",
    "    A = {\"conv1\": [], \"conv2\": [], \"fc1\": []}\n",
    "    Y = []\n",
    "    n = 0\n",
    "    for i, (imgs, labels) in enumerate(loader):\n",
    "        imgs = imgs.to(device)\n",
    "        logits, acts = model(imgs, return_acts=True)\n",
    "        A[\"conv1\"].append(acts[\"conv1\"].cpu().numpy().reshape(imgs.size(0), -1))\n",
    "        A[\"conv2\"].append(acts[\"conv2\"].cpu().numpy().reshape(imgs.size(0), -1))\n",
    "        A[\"fc1\"].append(acts[\"fc1\"].cpu().numpy())\n",
    "        Y.append(labels.numpy())\n",
    "        n += 1\n",
    "        if max_batches is not None and n >= max_batches:\n",
    "            break\n",
    "    for k in A:\n",
    "        A[k] = np.concatenate(A[k], axis=0)\n",
    "    Y = np.concatenate(Y, axis=0)\n",
    "    return A, Y\n",
    "\n",
    "acts, y_small = collect_activations(model, test_loader, max_batches=40)  # limit for speed/memory\n",
    "for layer in [\"conv1\",\"conv2\",\"fc1\"]:\n",
    "    X = acts[layer]\n",
    "    Xz = StandardScaler().fit_transform(X)\n",
    "    p2 = PCA(n_components=2, random_state=SEED)\n",
    "    Z = p2.fit_transform(Xz)\n",
    "    plt.figure()\n",
    "    # Use a subset of classes to avoid overplot clutter (optional)\n",
    "    idx = np.random.choice(len(Z), size=min(4000, len(Z)), replace=False)\n",
    "    plt.scatter(Z[idx,0], Z[idx,1], c=y_small[idx], s=6)\n",
    "    plt.title(f\"PCA — {layer} activations\")\n",
    "    plt.xlabel(\"PC1\"); plt.ylabel(\"PC2\")\n",
    "    plt.tight_layout(); plt.savefig(f\"results/06_pca_{layer}.png\", dpi=150); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f29880",
   "metadata": {},
   "source": [
    "## 8. Linear Probes (per layer)\n",
    "Fit a **logistic regression** on frozen layer activations to quantify separability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b30d5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_probe_acc(X_train, y_train, X_test, y_test):\n",
    "    scaler = StandardScaler()\n",
    "    Xz_tr = scaler.fit_transform(X_train)\n",
    "    Xz_te = scaler.transform(X_test)\n",
    "    clf = LogisticRegression(max_iter=200, multi_class=\"multinomial\")\n",
    "    clf.fit(Xz_tr, y_train)\n",
    "    return accuracy_score(y_test, clf.predict(Xz_te))\n",
    "\n",
    "# Collect activations for train and test (smaller subsets to keep runtime reasonable)\n",
    "acts_tr, y_tr_small = collect_activations(model, train_loader, max_batches=80)\n",
    "acts_te, y_te_small = collect_activations(model, test_loader,  max_batches=40)\n",
    "\n",
    "probe_scores = {}\n",
    "for layer in [\"conv1\",\"conv2\",\"fc1\"]:\n",
    "    acc_probe = linear_probe_acc(acts_tr[layer], y_tr_small, acts_te[layer], y_te_small)\n",
    "    probe_scores[layer] = acc_probe\n",
    "    print(f\"Linear probe accuracy — {layer}: {acc_probe:.4f}\")\n",
    "\n",
    "# Save a tiny CSV summary\n",
    "import csv\n",
    "with open(\"results/06_linear_probe_summary.csv\", \"w\", newline=\"\") as f:\n",
    "    w = csv.writer(f); w.writerow([\"layer\",\"probe_accuracy\"])\n",
    "    for k,v in probe_scores.items():\n",
    "        w.writerow([k, v])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c51c8b70",
   "metadata": {},
   "source": [
    "## 9. Sample Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f3ced6",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def sample_preds(model, loader, n=16):\n",
    "    model.eval()\n",
    "    imgs_all, y_all, yhat_all = [], [], []\n",
    "    for imgs, labels in loader:\n",
    "        logits = model(imgs.to(device)).cpu()\n",
    "        preds = logits.argmax(1)\n",
    "        imgs_all.append(imgs); y_all.append(labels); yhat_all.append(preds)\n",
    "        if sum(x.size(0) for x in imgs_all) >= n:\n",
    "            break\n",
    "    imgs = torch.cat(imgs_all,0)[:n]\n",
    "    y = torch.cat(y_all,0)[:n]\n",
    "    yhat = torch.cat(yhat_all,0)[:n]\n",
    "    return imgs, y.numpy(), yhat.numpy()\n",
    "\n",
    "imgs, y_true, y_hat = sample_preds(model, test_loader, n=16)\n",
    "import math\n",
    "cols = 8\n",
    "rows = math.ceil(len(imgs)/cols)\n",
    "plt.figure(figsize=(cols*1.2, rows*1.2))\n",
    "for i in range(len(imgs)):\n",
    "    plt.subplot(rows, cols, i+1)\n",
    "    plt.imshow(imgs[i,0], cmap=\"gray\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(f\"{y_true[i]}→{y_hat[i]}\", fontsize=8)\n",
    "plt.suptitle(\"Sample predictions (true→pred)\")\n",
    "plt.tight_layout(); plt.savefig(\"results/06_sample_preds.png\", dpi=150); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc2e799",
   "metadata": {},
   "source": [
    "## 10. Takeaways\n",
    "- As depth increases (**conv1 → conv2 → fc1**), **PCA projections** typically show **cleaner class separation**.\n",
    "- **Linear probes** quantify separability: later layers should provide higher probe accuracy.\n",
    "- This sets up comparisons to **perceptron/logistic baselines** and justifies moving to deeper CNNs."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
