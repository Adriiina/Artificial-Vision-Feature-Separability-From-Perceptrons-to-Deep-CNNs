{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b0267fe",
   "metadata": {},
   "source": [
    "# Artificial Vision & Feature Separability — 03 · Exemplar Models & GCM\n",
    "\n",
    "**Goal.** Implement and analyze **exemplar-based categorization**, focusing on the **Generalized Context Model (GCM)** and comparisons to **prototype** and **k-NN** across color datasets.\n",
    "**Outputs.** Log-loss/accuracy tables, confusion matrices, decision boundaries (2D PCA), and a sensitivity sweep over **similarity decay** and **distance metrics**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc13b766",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Reproducibility & Environment ---\n",
    "import os, random\n",
    "import numpy as np\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "os.makedirs(\"results\", exist_ok=True)\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "\n",
    "print(\"Seed set to\", SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c4e0f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Imports ---\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, log_loss, roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2b6308",
   "metadata": {},
   "source": [
    "## 1. Data\n",
    "Load `data/colors.csv` (RGB or Lab + label) or generate a balanced toy set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a82b6bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSV loader (preferred)\n",
    "csv_path = Path(\"data/colors.csv\")\n",
    "if csv_path.exists():\n",
    "    df = pd.read_csv(csv_path)\n",
    "    feat_cols = [c for c in df.columns if c.lower() in [\"r\",\"g\",\"b\",\"l\",\"a\",\"b\"]][:3]\n",
    "    assert len(feat_cols)==3, \"Expect 3 feature columns (RGB or Lab).\"\n",
    "else:\n",
    "    # Balanced toy colors\n",
    "    def make_toy_colors(n_per=250, noise=30, seed=SEED):\n",
    "        rng = np.random.default_rng(seed)\n",
    "        centers = {\n",
    "            \"red\":   np.array([220, 40, 40]),\n",
    "            \"green\": np.array([40, 220, 40]),\n",
    "            \"blue\":  np.array([40, 40, 220]),\n",
    "        }\n",
    "        X_list, y_list = [], []\n",
    "        for lab, c in centers.items():\n",
    "            Xc = rng.normal(c, noise, size=(n_per, 3)).clip(0,255)\n",
    "            X_list.append(Xc); y_list += [lab]*n_per\n",
    "        X = np.vstack(X_list).astype(np.float32)\n",
    "        y = np.array(y_list)\n",
    "        return pd.DataFrame({\"R\":X[:,0], \"G\":X[:,1], \"B\":X[:,2], \"label\":y})\n",
    "    df = make_toy_colors()\n",
    "    feat_cols = [\"R\",\"G\",\"B\"]\n",
    "\n",
    "X = df[feat_cols].values\n",
    "y = df[\"label\"].values\n",
    "labels = sorted(np.unique(y))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=SEED, stratify=y)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "Xz_train = scaler.fit_transform(X_train)\n",
    "Xz_test  = scaler.transform(X_test)\n",
    "\n",
    "print(\"Train/Test:\", X_train.shape, X_test.shape, \"Labels:\", labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e48faf22",
   "metadata": {},
   "source": [
    "## 2. Prototype & k-NN Baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f81ae69d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prototype (nearest class centroid in standardized space)\n",
    "centroids = {lab: Xz_train[y_train==lab].mean(axis=0) for lab in labels}\n",
    "def proto_predict(Xz):\n",
    "    preds = []\n",
    "    for row in Xz:\n",
    "        d = {lab: np.linalg.norm(row - mu) for lab, mu in centroids.items()}\n",
    "        preds.append(min(d, key=d.get))\n",
    "    return np.array(preds)\n",
    "\n",
    "pred_proto = proto_predict(Xz_test)\n",
    "acc_proto = accuracy_score(y_test, pred_proto)\n",
    "cm_proto = confusion_matrix(y_test, pred_proto, labels=labels)\n",
    "print(f\"Prototype — Acc: {acc_proto:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af0f5be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# k-NN (exemplar with uniform votes)\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(Xz_train, y_train)\n",
    "pred_knn = knn.predict(Xz_test)\n",
    "proba_knn = knn.predict_proba(Xz_test)\n",
    "acc_knn = accuracy_score(y_test, pred_knn)\n",
    "ll_knn = log_loss(y_test, proba_knn, labels=labels)\n",
    "print(f\"k-NN (k=5) — Acc: {acc_knn:.3f} | Log-loss: {ll_knn:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4685172f",
   "metadata": {},
   "source": [
    "## 3. Generalized Context Model (GCM)\n",
    "Similarity-based exemplar model.  \n",
    "Similarity \\( s(x, x_i) = \\exp(-c \\cdot d(x, x_i)) \\) with distance \\( d \\) as Minkowski-\\(p\\) (\\(p=1\\) Manhattan, \\(p=2\\) Euclidean)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c8a30e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def minkowski_distance(x, Y, p=2):\n",
    "    # x: (d,), Y: (n,d)\n",
    "    return np.power(np.abs(Y - x)**p, 1.0).sum(axis=1)**(1.0/p)\n",
    "\n",
    "def gcm_proba(Xz, Xz_train, y_train, labels, c=2.0, p=2):\n",
    "    proba = np.zeros((Xz.shape[0], len(labels)), dtype=float)\n",
    "    lab2idx = {lab:i for i,lab in enumerate(labels)}\n",
    "    for i, x in enumerate(Xz):\n",
    "        dist = minkowski_distance(x, Xz_train, p=p)\n",
    "        sim = np.exp(-c * dist)\n",
    "        for s, lab in zip(sim, y_train):\n",
    "            proba[i, lab2idx[lab]] += s\n",
    "        ssum = proba[i].sum()\n",
    "        if ssum > 0: proba[i] /= ssum\n",
    "    return proba"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e37f7d22",
   "metadata": {},
   "source": [
    "### 3.1 Hyperparameter Sweep (c, p) via Cross-Validation\n",
    "We pick \\(c\\in\\{0.5,1,2,4,8\\}\\) and \\(p\\in\\{1,2\\}\\) using 5-fold Stratified CV on the training set, minimizing **log-loss**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0940ef70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED)\n",
    "C_GRID = [0.5, 1.0, 2.0, 4.0, 8.0]\n",
    "P_GRID = [1, 2]\n",
    "\n",
    "best = {\"logloss\": np.inf, \"c\": None, \"p\": None}\n",
    "for cval in C_GRID:\n",
    "    for pval in P_GRID:\n",
    "        ll_list = []\n",
    "        for tr_idx, va_idx in cv.split(Xz_train, y_train):\n",
    "            Xtr, Xva = Xz_train[tr_idx], Xz_train[va_idx]\n",
    "            ytr, yva = y_train[tr_idx], y_train[va_idx]\n",
    "            proba = gcm_proba(Xva, Xtr, ytr, labels, c=cval, p=pval)\n",
    "            ll = log_loss(yva, proba, labels=labels)\n",
    "            ll_list.append(ll)\n",
    "        mean_ll = float(np.mean(ll_list))\n",
    "        if mean_ll < best[\"logloss\"]:\n",
    "            best.update({\"logloss\": mean_ll, \"c\": cval, \"p\": pval})\n",
    "\n",
    "print(\"Best GCM params:\", best)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "166025ef",
   "metadata": {},
   "source": [
    "### 3.2 Evaluate GCM (best params) on Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c3faa63",
   "metadata": {},
   "outputs": [],
   "source": [
    "proba_gcm = gcm_proba(Xz_test, Xz_train, y_train, labels, c=best[\"c\"], p=best[\"p\"])\n",
    "pred_gcm = np.array([labels[i] for i in np.argmax(proba_gcm, axis=1)])\n",
    "acc_gcm = accuracy_score(y_test, pred_gcm)\n",
    "ll_gcm = log_loss(y_test, proba_gcm, labels=labels)\n",
    "cm_gcm = confusion_matrix(y_test, pred_gcm, labels=labels)\n",
    "print(f\"GCM — Acc: {acc_gcm:.3f} | Log-loss: {ll_gcm:.3f} (c={best['c']}, p={best['p']})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b7272d",
   "metadata": {},
   "source": [
    "## 4. Logistic Regression Reference (probabilistic baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf489a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression(max_iter=1000, multi_class=\"multinomial\", random_state=SEED)\n",
    "logreg.fit(Xz_train, y_train)\n",
    "proba_lr = logreg.predict_proba(Xz_test)\n",
    "pred_lr = logreg.predict(Xz_test)\n",
    "acc_lr = accuracy_score(y_test, pred_lr)\n",
    "ll_lr = log_loss(y_test, proba_lr, labels=labels)\n",
    "cm_lr = confusion_matrix(y_test, pred_lr, labels=labels)\n",
    "print(f\"LogReg — Acc: {acc_lr:.3f} | Log-loss: {ll_lr:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2bfaa61",
   "metadata": {},
   "source": [
    "## 5. Results Overview\n",
    "We save confusion matrices and a small table for accuracy/log-loss comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44888f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "summary = [\n",
    "    (\"prototype\", accuracy_score(y_test, pred_proto), np.nan),\n",
    "    (\"knn_k5\", acc_knn, ll_knn),\n",
    "    (\"gcm\", acc_gcm, ll_gcm),\n",
    "    (\"logreg\", acc_lr, ll_lr),\n",
    "]\n",
    "with open(\"results/03_summary.csv\", \"w\", newline=\"\") as f:\n",
    "    w = csv.writer(f); w.writerow([\"model\",\"accuracy\",\"logloss\"]); w.writerows(summary)\n",
    "\n",
    "for name, cm in [(\"prototype\", cm_proto), (\"knn\", confusion_matrix(y_test, pred_knn, labels=labels)),\n",
    "                 (\"gcm\", cm_gcm), (\"logreg\", cm_lr)]:\n",
    "    plt.figure()\n",
    "    plt.imshow(cm, aspect=\"auto\")\n",
    "    plt.title(f\"Confusion — {name}\")\n",
    "    plt.xlabel(\"Pred\"); plt.ylabel(\"True\")\n",
    "    plt.xticks(range(len(labels)), labels, rotation=45)\n",
    "    plt.yticks(range(len(labels)), labels)\n",
    "    plt.colorbar(); plt.tight_layout(); plt.savefig(f\"results/03_confusion_{name}.png\", dpi=150); plt.show()\n",
    "\n",
    "print(\"Wrote results/03_summary.csv and confusion matrices.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a779961",
   "metadata": {},
   "source": [
    "## 6. Decision Boundaries (2D PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d18bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "p2 = PCA(n_components=2, random_state=SEED)\n",
    "Z_train = p2.fit_transform(Xz_train)\n",
    "Z_test  = p2.transform(Xz_test)\n",
    "\n",
    "def label_to_int(arr, labels):\n",
    "    mapping = {lab:i for i,lab in enumerate(labels)}\n",
    "    return np.vectorize(mapping.get)(arr)\n",
    "\n",
    "# For decision surfaces, fit simple surrogates on 2D space:\n",
    "log2d = LogisticRegression(max_iter=1000, multi_class=\"multinomial\", random_state=SEED).fit(Z_train, y_train)\n",
    "knn2d = KNeighborsClassifier(n_neighbors=5).fit(Z_train, y_train)\n",
    "\n",
    "xmin, ymin = Z_train.min(axis=0) - 1\n",
    "xmax, ymax = Z_train.max(axis=0) + 1\n",
    "xx, yy = np.meshgrid(np.linspace(xmin, xmax, 200), np.linspace(ymin, ymax, 200))\n",
    "grid = np.c_[xx.ravel(), yy.ravel()]\n",
    "\n",
    "for model, name in [(log2d, \"logistic\"), (knn2d, \"knn\")]:\n",
    "    pred = model.predict(grid).reshape(xx.shape)\n",
    "    plt.figure()\n",
    "    plt.contourf(xx, yy, label_to_int(pred, labels), alpha=0.3)\n",
    "    plt.scatter(Z_train[:,0], Z_train[:,1], c=label_to_int(y_train, labels), s=10, edgecolor='k', linewidth=0.2)\n",
    "    plt.title(f\"Decision — {name} (2D PCA)\")\n",
    "    plt.tight_layout(); plt.savefig(f\"results/03_boundary_{name}_2d.png\", dpi=150); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d2361bb",
   "metadata": {},
   "source": [
    "## 7. Sensitivity: Vary GCM Decay (c) at Fixed p\n",
    "We plot accuracy/log-loss on test as we vary \\(c\\) around the best value (holding \\(p\\) fixed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d71fa2e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "c_vals = [best[\"c\"]/4, best[\"c\"]/2, best[\"c\"], best[\"c\"]*2, best[\"c\"]*4]\n",
    "c_vals = [float(c) for c in c_vals if c>0]\n",
    "accs, lls = [], []\n",
    "for cval in c_vals:\n",
    "    proba_ = gcm_proba(Xz_test, Xz_train, y_train, labels, c=cval, p=best[\"p\"])\n",
    "    pred_ = np.array([labels[i] for i in np.argmax(proba_, axis=1)])\n",
    "    accs.append(accuracy_score(y_test, pred_))\n",
    "    lls.append(log_loss(y_test, proba_, labels=labels))\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(c_vals, accs, marker=\"o\")\n",
    "plt.xlabel(\"c (similarity decay)\"); plt.ylabel(\"Accuracy\"); plt.title(\"GCM Sensitivity — Accuracy\")\n",
    "plt.tight_layout(); plt.savefig(\"results/03_gcm_sensitivity_acc.png\", dpi=150); plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(c_vals, lls, marker=\"o\")\n",
    "plt.xlabel(\"c (similarity decay)\"); plt.ylabel(\"Log-loss\"); plt.title(\"GCM Sensitivity — Log-loss\")\n",
    "plt.tight_layout(); plt.savefig(\"results/03_gcm_sensitivity_ll.png\", dpi=150); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f118a0",
   "metadata": {},
   "source": [
    "## 8. Takeaways\n",
    "- GCM’s **decay (c)** and **distance metric (p)** materially affect performance; tuning by CV on log-loss is recommended.\n",
    "- Prototype is strong when classes are compact; k-NN captures fine detail but can overfit without CV for k.\n",
    "- Logistic sets a probabilistic linear baseline; exemplar methods often produce better **log-loss** when boundaries are complex."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
